{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preparation and Integration\n",
        "\n",
        "###Primary Key Identification\n",
        "1. Analyze both datasets and identify a Primary Key for integration.\n",
        "2. Justify your selection and highlight potential challenges in identifying a unique\n",
        "key."
      ],
      "metadata": {
        "id": "-mEvt4KQaatW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Data for Task 2..xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "df1 = pd.read_excel(file_path,sheet_name= 'Work Order Data')\n",
        "df2 = pd.read_excel(file_path,sheet_name= 'Repair Data')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwUADlYHrYlW",
        "outputId": "faa224a5-67ce-4ea4-f543-007b513bd109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis of Primary Key for Integration:\n",
        "Both datasets (Work Order Data and Repair Data) share three key columns:\n",
        "- **Primary Key**\n",
        "- **Order No**\n",
        "- **Segment Number**\n",
        "\n",
        "1. Identifying a Primary Key\n",
        "The \"Primary Key\" column appears in both datasets and has:\n",
        "- 500 entries in both datasets (No missing values).\n",
        "- Unique-looking values (e.g., \"SO0005588-1\") that seem to follow a structured format (likely combining \"Order No\" and \"Segment Number\").\n",
        "\n",
        "Thus, \"Primary Key\" is the best candidate for integration, as it exists in both datasets and appears to uniquely identify each row.\n",
        "\n",
        "2. Justification for Selection\n",
        "- Consistency: \"Primary Key\" is present in both datasets with the same formatting.\n",
        "- Uniqueness: Likely derived from \"Order No\" and \"Segment Number,\" making it specific to each work order.\n",
        "- Integration Feasibility: Both datasets have the same number of entries (500), meaning there's a one-to-one relationship."
      ],
      "metadata": {
        "id": "LKOT57wJc88l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Cleaning\n",
        "1. Load the datasets using Python.\n",
        "2. Inspect the column structure, and clean the data:\n",
        "    * Handle missing values, duplicates\n",
        "    * Format Correction - Consistent data types across dataset   \n",
        "    * Apply language translation if applicable.      \n",
        "3. Provide a brief summary of your data cleaning process.\n"
      ],
      "metadata": {
        "id": "ba4ISqf7dyN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Data Cleaning Process:\n",
        "\n",
        "1. Handled Missing Values -> Replaced missing values in:\n",
        "          - \"Cause\" (set to \"Unknown\")\n",
        "          - \"Correction\" (set to \"Not Provided\")\n",
        "          - \"Coverage\" (set to \"Not Available\")\n",
        "          - \"Actual Hours\" (filled with median value).\n",
        "\n",
        "2. Removed Duplicates-> Dropped 2 duplicate rows from df2 (Repair Data).\n",
        "\n",
        "3. Ensured Consistent Data Types:\n",
        "      - Converted \"Revenue\" and \"Cost\" to numeric.\n",
        "      - Transformed \"Invoice Date\" to datetime format."
      ],
      "metadata": {
        "id": "HGJMcADgKuYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find duplicate rows before dropping\n",
        "duplicates = df2[df2.duplicated(keep=False)]  # Keep=False shows all duplicate occurrences\n",
        "print(\"Duplicate Rows:\\n\", duplicates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkK5KIBm6eMO",
        "outputId": "a73eb8f3-2d67-44c0-e879-e45e66e4c69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate Rows:\n",
            "      Primary Key   Order No  Segment Number Coverage  Qty Part Manufacturer  \\\n",
            "270  SO0059080-2  SO0059080               2      NaN    1              PASE   \n",
            "271  SO0059080-2  SO0059080               2      NaN    1              PASE   \n",
            "326  SO0059284-4  SO0059284               4      NaN    1  DEVRE ENTERPRISE   \n",
            "327  SO0059284-4  SO0059284               4      NaN    1  DEVRE ENTERPRISE   \n",
            "\n",
            "                             Part Number       Part Description  Revenue  \\\n",
            "270    87630098                     CASE              ELBOW  45   5.5499   \n",
            "271    87630098                     CASE              ELBOW  45   5.5499   \n",
            "326  BOLT-Q                       TRINIT  BOLT BY QUARTER POUND   5.1399   \n",
            "327  BOLT-Q                       TRINIT  BOLT BY QUARTER POUND   5.1399   \n",
            "\n",
            "     Cost  Invoice Date  Actual Hours Segment Total $  \n",
            "270  3.1$         45219       12.8897      2612.8196$  \n",
            "271  3.1$         45219       12.8897      2612.8196$  \n",
            "326  1.3$         45286       43.7592      7569.1997$  \n",
            "327  1.3$         45286       43.7592      7569.1997$  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values\n",
        "\n",
        "# In df1 (Work Order Data):\n",
        "df1[\"Cause\"].fillna(\"Unknown\", inplace=True)  # Replace missing 'Cause' with 'Unknown'\n",
        "df1[\"Correction\"].fillna(\"Not Provided\", inplace=True)  # Replace missing 'Correction' with 'Not Provided'\n",
        "\n",
        "# In df2 (Repair Data):\n",
        "df2[\"Coverage\"].fillna(\"Not Available\", inplace=True)  # Replace missing 'Coverage' with 'Not Available'\n",
        "df2[\"Actual Hours\"].fillna(df2[\"Actual Hours\"].median(), inplace=True)  # Replace missing 'Actual Hours' with median value\n",
        "\n",
        "# Removing duplicates\n",
        "df2.drop_duplicates(inplace=True)  # Remove duplicates in df2\n",
        "\n",
        "# Ensure consistency in data types\n",
        "df2[\"Revenue\"] = pd.to_numeric(df2[\"Revenue\"], errors=\"coerce\")  # Convert 'Revenue' to numeric\n",
        "df2[\"Cost\"] = df2[\"Cost\"].replace(\"[\\$,]\", \"\", regex=True).astype(float)  # Convert 'Cost' to numeric\n",
        "\n",
        "# Convert 'Invoice Date' to proper datetime format\n",
        "df2[\"Invoice Date\"] = pd.to_datetime(df2[\"Invoice Date\"], origin='1899-12-30', unit='D')\n",
        "\n",
        "# Verify changes\n",
        "df1.info(), df2.info(), df2.duplicated().sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7SEisNYbh_X",
        "outputId": "21f390b8-c830-46a6-e73d-90e97748429a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 15 columns):\n",
            " #   Column                                 Non-Null Count  Dtype         \n",
            "---  ------                                 --------------  -----         \n",
            " 0   Primary Key                            500 non-null    object        \n",
            " 1   Order No                               500 non-null    object        \n",
            " 2   Segment Number                         500 non-null    int64         \n",
            " 3   Order Date                             500 non-null    datetime64[ns]\n",
            " 4   Manufacturer                           500 non-null    object        \n",
            " 5   Model                                  500 non-null    object        \n",
            " 6   Product Category                       500 non-null    object        \n",
            " 7   Model Year                             500 non-null    int64         \n",
            " 8   Serial Number                          500 non-null    object        \n",
            " 9   Meter 1 Reading                        500 non-null    float64       \n",
            " 10  Complaint                              500 non-null    object        \n",
            " 11  Cause                                  500 non-null    object        \n",
            " 12  Correction                             500 non-null    object        \n",
            " 13  Failure Condition - Failure Component  500 non-null    object        \n",
            " 14  Fix Condition - Fix Component          500 non-null    object        \n",
            "dtypes: datetime64[ns](1), float64(1), int64(2), object(11)\n",
            "memory usage: 58.7+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 498 entries, 0 to 499\n",
            "Data columns (total 13 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   Primary Key        498 non-null    object        \n",
            " 1   Order No           498 non-null    object        \n",
            " 2   Segment Number     498 non-null    int64         \n",
            " 3   Coverage           498 non-null    object        \n",
            " 4   Qty                498 non-null    int64         \n",
            " 5   Part Manufacturer  498 non-null    object        \n",
            " 6   Part Number        498 non-null    object        \n",
            " 7   Part Description   498 non-null    object        \n",
            " 8   Revenue            498 non-null    float64       \n",
            " 9   Cost               498 non-null    float64       \n",
            " 10  Invoice Date       498 non-null    datetime64[ns]\n",
            " 11  Actual Hours       498 non-null    float64       \n",
            " 12  Segment Total $    498 non-null    object        \n",
            "dtypes: datetime64[ns](1), float64(3), int64(2), object(7)\n",
            "memory usage: 54.5+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-180ba2f8b225>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df1[\"Cause\"].fillna(\"Unknown\", inplace=True)  # Replace missing 'Cause' with 'Unknown'\n",
            "<ipython-input-13-180ba2f8b225>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df1[\"Correction\"].fillna(\"Not Provided\", inplace=True)  # Replace missing 'Correction' with 'Not Provided'\n",
            "<ipython-input-13-180ba2f8b225>:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df2[\"Coverage\"].fillna(\"Not Available\", inplace=True)  # Replace missing 'Coverage' with 'Not Available'\n",
            "<ipython-input-13-180ba2f8b225>:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df2[\"Actual Hours\"].fillna(df2[\"Actual Hours\"].median(), inplace=True)  # Replace missing 'Actual Hours' with median value\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Integration\n",
        "- Merge the two datasets on the identified primary key to create a comprehensive view of the datasets.\n",
        "- Choose the appropriate type of join (inner, left, etc.), and justify your choice. Discuss the implications of using other join types in this context.\n"
      ],
      "metadata": {
        "id": "3Utv0CCIn4nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Cleaning Summary\n",
        "\n",
        "1. Handled Missing Values ->\n",
        "Filled missing values in key columns like \"Cause\", \"Correction\", and \"Coverage\" with appropriate default values (\"Unknown\", \"Not Provided\", etc.).Set numerical columns (\"Actual Hours\", \"Cost\") to 0 where needed.\n",
        "\n",
        "2. Removed Duplicates ->\n",
        "Ensured \"Primary Key\" was unique by dropping duplicate rows.\n",
        "\n",
        "3. Formatted Data Types ->\n",
        "Converted \"Cost\" and \"Segment Total $\" to numeric (removed $ signs).\n",
        "Converted \"Invoice Date\" to datetime format.Ensured \"Primary Key\" and \"Order No\" were stored as strings for consistent merging.\n",
        "\n",
        "4. Merged Datasets ->\n",
        "Used a LEFT JOIN on \"Primary Key\" to combine \"Work Order Data\" and \"Repair Data\", keeping all work orders even if repair data was missing."
      ],
      "metadata": {
        "id": "GvBqZZ7VXBrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why LEFT JOIN?\n",
        "- Keeps all Work Order Data (df1) - Every work order remains, even if no matching repair record exists.\n",
        "- Adds Repair Data (df2) when available - If a repair exists for the work order, it gets added.\n",
        "- Prevents Data Loss - If we used an INNER JOIN, we'd lose work orders that don't have a repair record.\n",
        "\n",
        "Implications of Other Join Types:\n",
        "\n",
        "1. Inner Join: This join will only keep records that exist in both datasets. If a work order does not have a corresponding repair record, it will be excluded. This could lead to loss of important work order data.\n",
        "\n",
        "2. Right Join: This join will keep all records from the Repair Data, even if there is no matching work order. This could introduce incomplete or orphaned repair records that may not be useful without the corresponding work order.\n",
        "\n",
        "3. Full Outer Join: This join will keep all records from both datasets, even if they don't have a match. While this ensures that no data is lost, it could lead to unnecessary null values in many fields where there is no match between work orders and repairs, making the dataset harder to analyze"
      ],
      "metadata": {
        "id": "c6qUoaqSabm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN5umLThYhhG",
        "outputId": "aa7f7b52-9751-4d05-f964-178ff7de37d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translated the correction column"
      ],
      "metadata": {
        "id": "nWZ5Z2kTCgqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# # Load the Work Order Data\n",
        "# file_path = \"Data for Task 2..xlsx\"  # Update with the correct path if needed\n",
        "# df1 = pd.read_excel(file_path, sheet_name=\"Work Order Data\")\n",
        "\n",
        "# Function to detect and translate non-English text\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        if pd.notna(text):  # Check if text is not NaN\n",
        "            return GoogleTranslator(source='auto', target='en').translate(text)\n",
        "        return text\n",
        "    except Exception:\n",
        "        return text  # Return original text if translation fails\n",
        "\n",
        "# Apply translation to the \"Correction\" column\n",
        "df1[\"Correction\"] = df1[\"Correction\"].apply(translate_text)\n",
        "\n",
        "# Save the translated dataset\n",
        "output_file = \"/content/drive/My Drive/Translated_Work_Order_Data.xlsx\"\n",
        "df1.to_excel(output_file, index=False)\n",
        "print(\"Translation completed. File saved as Translated_Work_Order_Data.xlsx\")\n",
        "\n",
        "# df1.to_excel(\"Translated_Work_Order_Data.xlsx\", index=False)\n",
        "# print(\"Translation completed. File saved as Translated_Work_Order_Data.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHwlH9CNYvDl",
        "outputId": "9b6a3ef6-db4b-4291-8f52-84890db66cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation completed. File saved as Translated_Work_Order_Data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"File Exists:\", os.path.exists(file_path))\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Translated_Work_Order_Data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGIkLyJs9xz3",
        "outputId": "95cd7a1f-dbdc-4201-be56-39d44a4152b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Exists: False\n",
            "   Primary Key   Order No  Segment Number Order Date Manufacturer Model  \\\n",
            "0  SO0005588-1  SO0005588               1 2022-04-30       PASEIH  6780   \n",
            "1  SO0005907-1  SO0005907               1 2022-04-30       PASEIH  6780   \n",
            "2  SO0006100-1  SO0006100               1 2022-04-30       PASEIH  6780   \n",
            "3  SO0006642-1  SO0006642               1 2022-04-30       PASEIH  6780   \n",
            "4  SO0018457-1  SO0018457               1 2022-04-30       PASEIH  6780   \n",
            "\n",
            "  Product Category  Model Year Serial Number  Meter 1 Reading  \\\n",
            "0             APPL           0     YFT042399        2531.0999   \n",
            "1             APPL           0     YFT042399        2531.0999   \n",
            "2             APPL           0     YFT042399        2531.0999   \n",
            "3             APPL           0     YFT042399        2531.0999   \n",
            "4             APPL           0     YFT042399        2531.0999   \n",
            "\n",
            "                                           Complaint    Cause  \\\n",
            "0  No cab heat, temp gauge dont get to operating ...  Unknown   \n",
            "1  No cab heat, temp gauge dont get to operating ...  Unknown   \n",
            "2  No cab heat, temp gauge dont get to operating ...  Unknown   \n",
            "3  No cab heat, temp gauge dont get to operating ...  Unknown   \n",
            "4  No cab heat, temp gauge dont get to operating ...  Unknown   \n",
            "\n",
            "                                          Correction  \\\n",
            "0  When I drove the device into the workshop, the...   \n",
            "1  When I drove the device into the workshop, the...   \n",
            "2  When I drove the device into the workshop, the...   \n",
            "3  When I drove the device into the workshop, the...   \n",
            "4  When I drove the device into the workshop, the...   \n",
            "\n",
            "  Failure Condition - Failure Component  \\\n",
            "0  No Heat - Cab, Not Achieving - Gauge   \n",
            "1  No Heat - Cab, Not Achieving - Gauge   \n",
            "2             Not Charging - Alternator   \n",
            "3                          Faulty - Fan   \n",
            "4              Oil Loss - Not Mentioned   \n",
            "\n",
            "                      Fix Condition - Fix Component  \n",
            "0                    No Component Mentioned - Added  \n",
            "1                    No Component Mentioned - Added  \n",
            "2   No Component Mentioned - No Component Mentioned  \n",
            "3  Tensioner - Removed, Crankshaft Pulley - Cleaned  \n",
            "4   No Component Mentioned - No Component Mentioned  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merged the two datasets"
      ],
      "metadata": {
        "id": "ferpgU-9Ckj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "# Merge the datasets using a LEFT JOIN on \"Primary Key\"\n",
        "df_merged = df1.merge(df2, on=\"Primary Key\", how=\"left\")\n",
        "\n",
        "# Save the merged dataset\n",
        "output_file = \"/content/drive/My Drive/Merged_Complete_Dataset.csv\"\n",
        "df_merged.to_csv(output_file, index=False)\n",
        "print(f\"File saved to: {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bN2x7-Bbh8k",
        "outputId": "088f5a50-2aaa-4e93-eb67-e6a316ef6795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "File saved to: /content/drive/My Drive/Merged_Complete_Dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filled the Cause Column"
      ],
      "metadata": {
        "id": "g7tVudgpGSF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Cause\"] = df.apply(\n",
        "    lambda row: f\"{row['Complaint']} | {row['Failure Condition - Failure Component']} | {row['Fix Condition - Fix Component']}\"\n",
        "    if row[\"Cause\"] == \"Unknown\" else row[\"Cause\"], axis=1\n",
        ")\n",
        "\n",
        "# Save the updated file\n",
        "df.to_excel(\"Cleaned_Merged_Data_Summarized_Task2.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "VSlpGC3_ELJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Translated the Correction Column\n",
        "- Merged the dataset\n",
        "- Filled the Cause Column\n",
        "- Total"
      ],
      "metadata": {
        "id": "jUAm_ekyH8ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Cleaned_Merged_Data_Summarized_Task2.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Function to translate text to English\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        if pd.notna(text):  # Check if text is not NaN\n",
        "            return GoogleTranslator(source='auto', target='en').translate(text)\n",
        "        return text\n",
        "    except Exception:\n",
        "        return text  # Return original text if translation fails\n",
        "\n",
        "# Apply translation to the \"Correction\" column\n",
        "df[\"Correction\"] = df[\"Correction\"].apply(translate_text)\n",
        "\n",
        "# Save the translated dataset as CSV\n",
        "df.to_csv(\"Translated_Cleaned_Merged_Data.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Translation completed. File saved as Translated_Cleaned_Merged_Data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-HksbjBf7rP",
        "outputId": "a958585e-3c82-4874-d1ab-82564328e6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation completed. File saved as Translated_Cleaned_Merged_Data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4rLg782bh3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}